{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRIVI FEATURES su .txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stampa feature di un DataFrame del tipo: <br>\n",
    "indice  |  feature  |  tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature types have been saved to 'data/features_info_195.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Prompt the user for the relative CSV file path\n",
    "csv_path = \"data/speed_dating.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(csv_path):\n",
    "    raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_path, encoding=\"latin1\")\n",
    "\n",
    "# Compute maximum widths for index, feature name, and dtype strings\n",
    "max_index_width   = len(str(len(df.columns) - 1))\n",
    "max_feature_width = max(len(col) for col in df.columns)\n",
    "max_dtype_width   = max(len(str(dtype)) for dtype in df.dtypes)\n",
    "\n",
    "# Prepare header (optional)\n",
    "header = (\n",
    "    f\"{'Idx'.ljust(max_index_width)}\\t\"\n",
    "    f\"{'Feature'.ljust(max_feature_width)}\\t\"\n",
    "    f\"{'Dtype'.ljust(max_dtype_width)}\\n\"\n",
    ")\n",
    "separator = \"-\" * (max_index_width + max_feature_width + max_dtype_width + 8) + \"\\n\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Write the aligned feature types into a text file\n",
    "output_file = \"data/features_info_123.txt\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(header)\n",
    "    f.write(separator)\n",
    "    for i, (col, dtype) in enumerate(df.dtypes.items()):\n",
    "        line = (\n",
    "            f\"{str(i).ljust(max_index_width)}\\t\"\n",
    "            f\"{col.ljust(max_feature_width)}\\t\"\n",
    "            f\"{str(dtype).ljust(max_dtype_width)}\\n\"\n",
    "        )\n",
    "        f.write(line)\n",
    "\n",
    "print(f\"Feature types have been saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stampa label del DataFrame - solo label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "csv_path = \"data/speed_dating.csv\"\n",
    "output_file = \"data/features_labels_123.txt\"\n",
    "df = pd.read_csv(csv_path, encoding=\"latin1\")\n",
    "\n",
    "# write on the txt file all the attribute features\n",
    "with open(output_file, \"w\") as f:\n",
    "    for col in df.columns:\n",
    "        f.write(f\"{col}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genera features_to_keep.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Richiedi il percorso relativo del file CSV\n",
    "csv_path = \"data/speed_dating.csv\"\n",
    "\n",
    "# Verifica che il file esista\n",
    "if not os.path.exists(csv_path):\n",
    "    raise FileNotFoundError(f\"File non trovato: {csv_path}\")\n",
    "\n",
    "# Carica il CSV in un DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Calcola la larghezza massima dei nomi delle feature per un allineamento uniforme\n",
    "max_width = max(len(col) for col in df.columns)\n",
    "\n",
    "# Assicurati che la cartella di output esista\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Scrivi il file di testo con le feature e il flag 'y'\n",
    "output_file = \"data/features_to_keep.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, col in enumerate(df.columns):\n",
    "        # Utilizza ljust() per allineare il nome della feature a sinistra con una larghezza fissa\n",
    "        # e separa con un tab la flag 'y'\n",
    "        f.write(f\"{str(i).rjust(3)}\\t{col.ljust(max_width)}\\t y\\n\")\n",
    "\n",
    "print(f\"Il file di selezione feature è stato salvato in: '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check di consistenza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistenza 'race' == 'race_o' → \"samerace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conta quante righe hanno attributo 'race' e 'race_o' con valori non nulli\n",
    "# dopodichè controlla (fra queste righe) se 'race' == 'race_o' e se la feature \"samerace\" è ben settata\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Filtra le righe in cui 'race' e 'race_o' non sono nulli\n",
    "non_null_rows = df[df['race'].notna() & df['race_o'].notna()]\n",
    "print(f\"Numero di righe con 'race' e 'race_o' non nulli: {len(non_null_rows)}\")\n",
    "\n",
    "# 2. Tra queste righe, conta quante hanno 'race' == 'race_o'\n",
    "same_race_rows = non_null_rows[non_null_rows['race'] == non_null_rows['race_o']]\n",
    "print(f\"Numero di righe in cui 'race' == 'race_o': {len(same_race_rows)}\")\n",
    "\n",
    "# 3. Verifica se 'samerace' è coerente\n",
    "# Assumiamo che:\n",
    "#    - se 'race' == 'race_o' allora 'samerace' debba essere 1\n",
    "#    - se 'race' != 'race_o' allora 'samerace' debba essere 0\n",
    "# Consideriamo solo le righe in cui 'samerace' non è nullo\n",
    "valid_rows = non_null_rows[non_null_rows['samerace'].notna()]\n",
    "\n",
    "# Rileva le righe in cui 'samerace' non è impostato correttamente\n",
    "inconsistent = valid_rows[\n",
    "    ((valid_rows['race'] == valid_rows['race_o']) & (valid_rows['samerace'] != 1)) |\n",
    "    ((valid_rows['race'] != valid_rows['race_o']) & (valid_rows['samerace'] != 0))\n",
    "]\n",
    "print(f\"Numero di righe con 'samerace' non coerente: {len(inconsistent)}\")\n",
    "\n",
    "if len(inconsistent) == 0:\n",
    "    print(\"La feature 'samerace' è ben settata.\")\n",
    "else:\n",
    "    print(\"La feature 'samerace' presenta delle incongruenze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistenza 'decision' == 'decision_o' → \"match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Consider only rows where 'decision', 'decision_o' and 'match' are not null\n",
    "valid_rows = df[df['decision'].notna() & df['decision_o'].notna() & df['match'].notna()]\n",
    "\n",
    "# Define the condition:\n",
    "# - If both 'decision' and 'decision_o' are 1, then 'match' must be 1.\n",
    "# - In all other cases, 'match' must be 0.\n",
    "inconsistent_rows = valid_rows[\n",
    "    ((valid_rows['decision'] == 1) & (valid_rows['decision_o'] == 1) & (valid_rows['match'] != 1)) |\n",
    "    (~((valid_rows['decision'] == 1) & (valid_rows['decision_o'] == 1)) & (valid_rows['match'] != 0))\n",
    "]\n",
    "\n",
    "print(f\"Total valid rows: {len(valid_rows)}\")\n",
    "print(f\"Number of inconsistent rows: {len(inconsistent_rows)}\")\n",
    "if len(inconsistent_rows) == 0:\n",
    "    print(\"The 'match' column is correctly set based on 'decision' and 'decision_o'.\")\n",
    "else:\n",
    "    print(\"Inconsistencies found in the 'match' column. Review these rows:\")\n",
    "    print(inconsistent_rows[['decision', 'decision_o', 'match']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistenza discretizzazione di 'interests_correlate' in 'd_interests_correlate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Definisci i bin e le etichette per la discretizzazione\n",
    "bins = [-1, 0, 0.33, 1]\n",
    "labels = ['[-1-0]', '[0-0.33]', '[0.33-1]']\n",
    "\n",
    "# Discretizza la colonna 'interests_correlate'\n",
    "# include_lowest=True assicura che il valore minimo (-1) venga incluso nel primo bin\n",
    "df['d_interests_correlate_check'] = pd.cut(df['interests_correlate'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Calcola i conteggi per ogni categoria (includendo anche i NaN)\n",
    "discretized_counts = df['d_interests_correlate_check'].value_counts(dropna=False).sort_index()\n",
    "nan_count = df['interests_correlate'].isna().sum()\n",
    "\n",
    "print(\"Conteggi ottenuti dalla discretizzazione:\")\n",
    "print(discretized_counts)\n",
    "print(f\"\\nNumero di NaN in 'interests_correlate': {nan_count}\")\n",
    "\n",
    "# --- Confronto con i valori attesi ---\n",
    "# Valori attesi:\n",
    "# [-1-0]      2384\n",
    "# [0-0.33]    3063\n",
    "# [0.33-1]    2931\n",
    "# NaN         158\n",
    "\n",
    "expected_counts = {\n",
    "    '[-1-0]': 2384,\n",
    "    '[0-0.33]': 3063,\n",
    "    '[0.33-1]': 2931,\n",
    "    'NaN': 158\n",
    "}\n",
    "\n",
    "# Estrai i conteggi per le categorie discretizzate\n",
    "obtained = {\n",
    "    '[-1-0]': int(discretized_counts.get('[-1-0]', 0)),\n",
    "    '[0-0.33]': int(discretized_counts.get('[0-0.33]', 0)),\n",
    "    '[0.33-1]': int(discretized_counts.get('[0-0.33]', 0)) if '[0-0.33]' not in discretized_counts else int(discretized_counts.get('[0.33-1]', 0))\n",
    "}\n",
    "\n",
    "# Nota: Per gestire i NaN, usiamo:\n",
    "obtained_nan = nan_count\n",
    "\n",
    "print(\"\\nConfronto con i valori attesi:\")\n",
    "for key in ['[-1-0]', '[0-0.33]', '[0-0.33]', '[0-0.33]']:\n",
    "    # Evitiamo duplicati se ci fossero\n",
    "    pass\n",
    "\n",
    "# Più semplicemente, stampiamo il confronto riga per riga:\n",
    "print(f\"[-1-0]: ottenuto {int(discretized_counts.get('[-1-0]', 0))}, atteso {expected_counts['[-1-0]']}\")\n",
    "print(f\"[0-0.33]: ottenuto {int(discretized_counts.get('[0-0.33]', 0))}, atteso {expected_counts['[0-0.33]']}\")\n",
    "print(f\"[0.33-1]: ottenuto {int(discretized_counts.get('[0.33-1]', 0))}, atteso {expected_counts['[0.33-1]']}\")\n",
    "print(f\"NaN:       ottenuto {obtained_nan}, atteso {expected_counts['NaN']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrotondamento/Clipping per post imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Verifica e Pulizia Post-Imputazione (Arrotondamento/Clipping) ---\n",
    "print(\"--- Arrotondamento/Clipping Post-Imputazione ---\")\n",
    "print(\"Valori unici PRIMA dell'arrotondamento:\")\n",
    "for col in cols_to_encode:\n",
    "     if col in df_imputed_RF.columns:\n",
    "         # Usa try-except perché .unique() potrebbe fallire se la colonna non esiste\n",
    "         try:\n",
    "            print(f\"{col}: {df_imputed_RF[col].unique()}\")\n",
    "         except KeyError:\n",
    "             pass # La colonna non era nel df imputato\n",
    "\n",
    "# Arrotonda e forza nel range valido le colonne originariamente categoriche\n",
    "for col in cols_to_encode:\n",
    "     if col in df_imputed_RF.columns:\n",
    "        original_map = encoding_maps.get(col) # Prendi la mappa originale\n",
    "        if original_map: # Assicurati che esista una mappa per questa colonna\n",
    "            min_val = min(original_map.values())\n",
    "            max_val = max(original_map.values())\n",
    "\n",
    "            # Controlla se la colonna è float (potrebbe essere int se non c'erano NaN originali)\n",
    "            if pd.api.types.is_float_dtype(df_imputed_RF[col]):\n",
    "                df_imputed_RF[col] = df_imputed_RF[col].round() # Arrotonda\n",
    "\n",
    "            # Converti in intero e fai il clipping in ogni caso per sicurezza\n",
    "            df_imputed_RF[col] = df_imputed_RF[col].astype(int).clip(min_val, max_val)\n",
    "\n",
    "\n",
    "print(\"\\nValori unici DOPO arrotondamento/clipping:\")\n",
    "for col in cols_to_encode:\n",
    "     if col in df_imputed_RF.columns:\n",
    "         try:\n",
    "             print(f\"{col}: {df_imputed_RF[col].unique()}\")\n",
    "         except KeyError:\n",
    "             pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check su sole righe con NaN, data una precisa feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_to_check= 'age'\n",
    "nan_indices = df[df[attribute_to_check].isna()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.age_o.dtype)\n",
    "print(\"----------RF----------------\")\n",
    "df_imputed_RF.loc[nan_indices, attribute_to_check].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------def----------------\")\n",
    "df_imputed_def.loc[nan_indices, attribute_to_check].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stampa grafico di quanti NaN nelle righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola il numero di NaN per ogni riga\n",
    "nan_counts = df.isna().sum(axis=1)\n",
    "\n",
    "# Calcola la distribuzione delle frequenze e ordina per indice\n",
    "freq = nan_counts.value_counts().sort_index()\n",
    "\n",
    "# Crea il bar plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = sns.barplot(x=freq.index, y=freq.values, palette='viridis')\n",
    "\n",
    "# Etichette e titolo\n",
    "plt.xlabel(\"Number of Missing Values per Row\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Missing Values per Row\")\n",
    "\n",
    "# Aggiungi le annotazioni con il valore assoluto sopra ogni barra\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height())}',\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center',\n",
    "                xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['gender', 'race', 'race_o', 'field', 'samerace']\n",
    "features_to_OHEncode = ['race', 'race_o', 'field']\n",
    "\n",
    "# converti le colonne il cui dtype è float o category in int\n",
    "for col in features_to_OHEncode:\n",
    "    if col in df_imputed_MF.columns and df_imputed_MF[col].dtype in [np.float64, 'category']:\n",
    "        df_imputed_MF[col] = df_imputed_MF[col].round().astype(int)\n",
    "        print(f\"Colonna '{col}' convertita in int.\")\n",
    "\n",
    "# --- 2. Applicazione del One-Hot Encoding con drop='first' ---\n",
    "# pd.get_dummies è la funzione da usare.\n",
    "# - df_imputed_MF: il DataFrame da trasformare.\n",
    "# - columns: la lista delle colonne su cui applicare la codifica.\n",
    "# - drop_first=True: elimina la prima categoria per ogni feature per evitare la multicollinearità.\n",
    "# - dtype=int: opzionale, per avere 0 e 1 come interi invece che float.\n",
    "df_OHEncoded = pd.get_dummies(\n",
    "    df_imputed_MF,\n",
    "    columns=features_to_OHEncode,\n",
    "    drop_first=True,\n",
    "    dtype=int\n",
    ")\n",
    "\n",
    "print(\"\\n--- DataFrame dopo One-Hot Encoding (df_OHEncoded) ---\")\n",
    "print(df_OHEncoded.head()) # Mostra le prime righe del nuovo DataFrame\n",
    "print(f\"\\nNuove dimensioni: {df_OHEncoded.shape}\")\n",
    "print(f\"\\nNuove colonne: {df_OHEncoded.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dtype categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['gender', 'race', 'race_o', 'field', 'samerace']\n",
    "\n",
    "for col in categorical_features:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\nColonna: {col}\")\n",
    "        print(f\"Tipo: {df[col].dtype}\")\n",
    "        print(df[col].value_counts(dropna=False).sort_index())\n",
    "    else:\n",
    "        print(f\"Colonna '{col}' non trovata nel DataFrame.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envDMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
